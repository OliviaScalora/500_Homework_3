---
title: "The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol"
author: "Olivia Scalora and Briana Cervantes"
date: "11/22/2021"
output:
  html_document:
    toc: true 
    toc_float: true 
    toc_depth: 6
    theme: yeti
    highlight: textmate
    code_folding: "hide"
    css: style.css
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(aod)
library(ggplot2)
library(rms)
library(gmodels)
library(boot)
library(DAAG)
library(ROCR)
library(tidyr)
library(dplyr)
library(MASS)
library(rsq)
library(kableExtra)
library(tidyverse) #for ggplot
#library(sf) #for maps
#library(cowplot) #for plotgrid
library(classInt)#for jenks breaks
library(rgdal)
library(RColorBrewer)
library(broom)
library(maptools)
library(spdep)
library(spgwr)
library(rgeos)
library(ape)
library(tmap)
library(sp)
library(spatialreg)
library(kableExtra)
library(knitr)
#install.packages('DT')
library(DT)

mydata <- read.csv("https://raw.githubusercontent.com/OliviaScalora/500_Homework_3/main/Logistic Regression Data.csv")
head(mydata)

```

# INTRODUCTION

Approximately one person every 51 minutes dies in motor vehicle crashes that involve an alcohol-impaired driver, according to the US Department of Transportation. In addition to loss of life and other major injuries, the economic impact of drunk driving is estimated to be more than $59 billion annually, according to a study recently conducted by the National Highway Traffic Safety Administration. The goal of this analysis is to use logistic regression to identify predictors of car crashes related to drunk driving using R and a data set from the City of Philadelphia which contains all 53,260 car accidents in Philadelphia from 2008 - 2012. The data set is then filtered to include only the 43,364 observations that occurred in Philadelphia residential block groups. The crashes are geocoded, allowing us to spatially join our crash data to the 2000 Census block group level data set. Each crash point will then include the median household income and percent of individuals with at least a bachelor's degree in the block group where the crash took place. The categorical variables contained in our original data set are all additional details about each car crash, which we can hope will have some level of association with whether or not the crash involved a drunk driver. The predictors joined from the census data are continuous variables that are more indicative of the area in which the crash occurred as opposed to the details of the crash itself. We are interested in the correlation between block group level variables that indicate wealth and education and drunk driving occurrences. 

The original car crash data set contains the following variables.   

-   **`CRN`**  Crash Record Number  
-   **`DRINGING_D`**  Drinking driver indicator (1 = Yes, 0 = No)    
-   **`COLLISION`**  Collision category that defines the crash (0 = Non collision, 1 = Rear end, 2 = Head on, 3 = Rear to rear, 4 = Angle, 5 = Sideswipe(same direct.), 6 = Sideswipe (opposite direct.), 7 = Hit fixed object, 8 = Hit pedestrian, 9 = Other or unknown)  
-   **`FATAL_OR_M`**  Crash resulted in a fatality or major injury (1 = Yes, 0 = No)  
-   **`OVERTURNED`**  Crash involved an overturned vehicle (1 = Yes, 0 = No)  
-   **`CELL_PHONE`**  Driver was using cell phone (1 = Yes, 0 = No)  
-   **`SPEEDING`**  Crash involved speeding car (1 = Yes, 0 = No)  
-   **`AGGRESSIVE`**  Crash involved aggressive driving (1 = Yes, 0 = No)  
-   **`DRIVER1617`**  Crash involved at least one driver who was 16 or 17 years old(1 = Yes, 0 = No)  
-   **`DRIVER56PLUS`**  Crash involved at least one driver who was at least 65 years old(1 = Yes, 0 = No)  

After joined with the Census block group data, we gain the following variables.   

-   **`AREAKEY`**  ID of the Census Block Group where the crash took place  
-   **`PCTBACHMOR`**  % of individuals 25 years of age or older who have at least a bachelor's degree in the Census Block Group where the crash took place  
-   **`MEDHHINC`**  Median household income in the Census block Group where the crash took place  



# METHODS

In OLS regression, the dependent variable is continuous and hopefully
normal. And the beta coefficients are interpreted as the amount that
dependent variable changes when the predictor increases by 1 unit. For
example, $\beta^1$ is the amount by which variable Y (our dependent
variable) changes when $x^1$ increases by 1 unit. But when dealing with
a binary $Y$ that has only two outcomes, 0 and 1, a one unit increase in
$x^1$ resulting in an $\beta^1$ increase in $Y$ makes no sense. Our
binary dependent variable can only ever change from 0 to 1, or from 1 to
0. Further, even when trying to get around the issue of predicting $Y$
and instead predicting the probability that $Y=1$, linear regression
predicts values of $Y$ that range between $-\infty$ to $+\infty$. This
also makes little sense, because probabilities must range between 0 and
1, and this linear model does not have such constraints.

If we are able to stay within the range of 0 to 1, we can not only talk
in terms of probability, but also in terms of odds which can be more
approachable for most people. While probability is calculated as the
number of desirable outcomes divided by the number of all possible
outcomes, odds are the amount of desirable outcomes divided by the
amount of the undesirable outcomes. In Logistic Regression, odds can be
calculated as $\frac{P(Y=1)}{1-P(Y=1)}$.

We can also look at odd ratios, or the ratio of odds of an outcome under
one set of conditions compared to the odds of an outcome under another
set of conditions. In multiple Logistic Regression, we can calculate the
odds ratio by exponentiating the beta coefficients from the regression.
The odds ratio can be interpreted as the extent to which the odds of
$Y=1$ changes as the predictor increases by 1 unit.

In our first logistic model, we included the following predictors:

-   whether crash resulted in fatality of major injury(`FATAL_OR_M`)  
-   whether crash involved an overturned vehicle - whether driver was
    using a cellphone (`CELL_PHONE`)    
-   whether crash involved speeding car (`SPEEDING`)  
-   whether crash involved aggressive driving (`AGGRESSIVE`)  
-   whether crash involved at least one drive who was 16 or 17 years old
    (`DRIVER1617`)    
-   whether crash involved at least one driver who was at least 65 year
    old (`DRIVER65PLUS`)    
-   percent of individuals 25 years of age or older who have at least a
    bachelors degree in the census block group where the crash took place (`PCTBACHMOR`)    
-   median household income in the census block group where the crash
    took place (`MEDHHINC`)  

Below are the equations in both the Logit form and the Logistic form.

**Logit Regression Equation**

$$\ln(\frac{P(DRINKING\_D=1)}{1-P(DRINKING\_D=1)}) = \beta_0+\beta_{FATAL\_OR\_M}+\beta_{OVERTURNED}+\beta_{CELLPHONE}+\beta_{SPEEDING}+\beta_{AGGRESSIVE}+\beta_{DRIVER1617}+\beta_{DRIVER65PLUS}+\beta_{PCTBACHMOR}+\beta_{MEDHHINC}+\epsilon$$

In the Logit form, the equation has the log odds of a crash involving a
drunk driver on the left side - that is what we are predicting, where 1
is yes and 0 is no. On the right, is the linear function of the
predictor variables as listed above. And $\epsilon$ is the error term.

**Logistic Regression Equation**

$$P(DRINKING\_D=1) = \frac{e^{\beta_0+\beta_{FATAL\_OR\_M}+\beta_{OVERTURNED}+\beta_{CELLPHONE}+\beta_{SPEEDING}+\beta_{AGGRESSIVE}+\beta_{DRIVER1617}+\beta_{DRIVER65PLUS}+\beta_{PCTBACHMOR}+\beta_{MEDHHINC}}}{1+e^{\beta_0+\beta_{FATAL\_OR\_M}+\beta_{OVERTURNED}+\beta_{CELLPHONE}+\beta_{SPEEDING}+\beta_{AGGRESSIVE}+\beta_{DRIVER1617}+\beta_{DRIVER65PLUS}+\beta_{PCTBACHMOR}+\beta_{MEDHHINC}}}$$

The Logistic Regression Equation is essentially the Logit equation
reorganized to solve for $p$, or the $P(Y=1)$. Because the Logistic
Regression equation has been reorganized to solve for $p$, we can use it
to estimate the probability of $Y=1$ directly, instead of estimating the
log odds of $Y=1$, as the Logit function does.

Logistic Regression acts as a translator function of a linear regression
that predicts probability, to get around the issue that kind of
regression poses. In the explanation above, though the range of possible
probabilities (from $-\infty$ to $+\infty$) doesn't make sense,
intuitively there are somethings we can glean from it. If the linear
probability model predicts a 1.10 = 110% chance of $Y=1$, then we might
expect there would be a very large probability of $Y$ actually equaling
1. And if the predicted probability were negative, e.g. -0.50 = -50%
chance of $Y=1$, then we would expect there to be a small probability of
$Y$ actually being equal to 1. While the range of probabilities provided
by a linear probability model do not make sense, Logistic Regression
acts a translator function, translating those outputs into a range that
does makes sense, a range of 0 to 1.

For example, Logistic Regression would translate values as such: the
closer the $\hat{y}$ value from the linear probability regression model
is to $-\infty$, the closer the predicted probability is to 0. And, the
closer the $\hat{y}$ value from the linear probability regression model
is to $+\infty$, the closer the predicted probability is to 1. This
leave us with a range from 0 to 1, where no predicted probabilities are
greater than 1 or less than 0.

Under logistic regression we test our hypothesis for each predictor. Our
null hypothesis is $H_0: \beta_i = 0 (OR_i = 1)$ and our alternative
hypothesis is $H_a:\beta_i \neq 0 (OR_i \neq 1)$. Additionally, our null
and alternative hypothesis include the $OR$ because most statisticians
prefer to work with odds ratios. Odds ratios can be calculated by
exponentiation the coefficients.

In logistic regression, when we take the quantity
$\frac{\hat{\beta_i}-E(\hat{\beta_i})}{\sigma_{\hat{\beta_i}}} = \frac{\hat{\beta_i}-0}{\sigma_{\hat{\beta_i}}} = \frac{\hat{\beta_i}}{\sigma_{\hat{\beta_i}}}$,
the quantity has a standard normal distribution. That means,
$\frac{\hat{\beta_i}}{\sigma_{\hat{\beta_i}}}=z$, and in the context of
Logistic Regression, this quantity is sometimes called the Wald
Statistic.

When thinking about the quality of our logistic regression model's fit,
we can calculate an R-squared value, but it's not very useful and does
not have the same interpretation as in OLS. This means that in Logistic
Regression we cannot interpret R-squared as the % of variance explained
by the model, like we can in OLS. The only interpretation we can make is
that the higher the R-squared, the better.

However, just as with other models, we can use the Akaike Information
Criterion to compare different logistic models, or even compare logistic
models to other kinds of models. The Akaike Information Criterion, or
AIC, evaluates the relative quality of statistical models by estimating
prediction error. The AIC is calculated using the maximum likelihood
estimate and the number of predictors used in the model. This means that
in comparing the quality of two statistical models, the AIC penalizes
the model for adding additional predictors. The lower the AIC, the
better model, if there is a difference in AIC of least 3 units. [UCLA,
Lecture 20: Model
Selection](http://www.stat.ucla.edu/~rgould/120w05/handouts/lect20.pdf)

For Logistic Regression we can also look at the specificity, sensitivity
and the misclassification rate. Sensitivity is calculated as:
$$\frac{True Positives}{(True Positives + False Positives)}$$
Specificity is calculated as:
$$\frac{True Negatives}{(True Negatives + False Negatives)}$$ And the
misclassification rate is calculated as:
$$\frac{False Negatives+False Positives}{(True Negatives+ True Positives + False Positives+ False Negatives)}$$
Sensitivity and specificity are essentially the rate at which we
correctly predict True Positives and True Negatives. Ideally both the
sensitivity and specificity will be high. The misclassification rate
however, is the overall rate that incorrectly predict both 1 and 0.
Ideally, the misclassification rate will be low.

In Logistic Regression, the fitted or predicted values of $y$, i.e.
$\hat{y}$, are calculated as the probability that $Y=1$. Ideally, our
model will be able to predict a high probability of $Y=1$ when Y is
actually 1. And ideally, our model will be able to predict a low
probability of $Y=1$ when Y is actually 0, meaning it's not 1. But
ultimately, we decide what is a high (enough) probability to be
considered a prediction of $Y=1$ and we decide what is low (enough) to
be considered a prediction of $Y\neq1$. The decision is where to place
the cutoff threshold. For example, if we decide that the cutoff
threshold should be 0.50, then any $\hat{y}$ \< 0.50 will be considered
a prediction of $Y\neq1$ and any $\hat{y}$ \> 0.50 will be considered a
prediction of $Y=1$.

What we correctly predict as $Y=1$ is called a True Positive and what we
incorrectly predict as $Y=1$ is called a False Positive. Together those
two values are used to calculate the Sensitivity, or the True Positive
rate. Similarly, what we correctly predict as $Y\neq1$ is called a True
Negative and what we incorrectly predict as $Y\neq1$ is called a False
Negative. Together those two values are used to calculate the
Specificity, or the True Negative rate. And lastly, all four values,
True Positive, True Negative, False Negative, and False Positive are
used to calculate the misclassification rate.

As noted above, which $\hat{y}$ values are considered to be correctly or
incorrectly predicted depends on where we set our cutoff threshold. Each
threshold has it's own upsides and downsides. A threshold of 0.06 may
result in a high True Positive rate, but it may also result in a high
False Positive Rate. Essentially, in this scenario, at the cost of counting
most $\hat{y}$ as predicting $Y=1$, you might correctly predict all of
the instances where y is actually 1, but you will also incorrectly
predict where y is actually 0.

To help in making the best decision of where to place the cutoff
threshold, we can use an ROC curve. The ROC curve shows the quality of
predictions of various thresholds by plotting the True Positive Rate
(Sensitivity) versus the False Positive Rate for every possible cut-off
threshold. The best threshold can be determined by optimizing
sensitivity and specificity. In the case that the model's use benefits
from having both sensitivity and specificity maximized, we can use the
Youden Index to identify an optimal threshold mathematically.
Alternatively, given that specificity and sensitivity should be
maximized, a threshold can be determined by identifying the point of the
curve where it has the minimum distance from the upper left corner of
the graph, i.e. where both specificity and sensitivity are equal to 1.
We will be using the latter approach in R.

Another helpful metric for determining the quality of our model is the
area under the ROC curve (AUC). The value of the AUC should be above
0.60 if the model is any use at all. An AUC between 0.50 and 0.60 is a
fail, between .60 and .70 is poor, between 0.70 and 0.80 is fair,
between .80 and .90 is good, and in some cases an AUC of .90 and 1 is
excellent. Although depending on the model's use, an AUC of 1 might not
be good, because it would mean that the model is overfit and would not
predict well on unseen data. Additionally, in real world cases, an AUC
greater than 0.7 is generally good.

The first assumption of Logistic Regression is that our dependent
variable be binary. Additionally, our observations must be independent
of each other, there should be no severe multicollinearity. Further
unlike in OLS, Logistic Regression does not require the assumption that
there needs to be a linear relationship between the dependent variable
and each predictor, nor does it assume homoscedasticity or that the
residuals be normal. However, larger samples are needed for Logistic
Regression because Maximum Likelihood Estimation is used instead of
least squares to estimate regression coefficients. We need at least 50
observations per predictor for Logistic Regression, compared to only 10
per predictor for OLS.

Before we run our Logistic Regression Model, it is a good idea to run
cross-tabulations between the dependent variable and binary predictors,
to see whether there is an association between them. In doing so, we may
identify a "perfect prediction problem" where only one value of a
predictor is associated with only one value of the response variable. If
this problem were identified, we would simply remove the predictor for
which this is a problem and continue with the model.

Additionally, the Chi-Square test is the appropriate statistical test
for examining the association between two categorical variables. In the
Chi-Square test, our null and alternative hypotheses are as such:   


<a name="chi" class="isDisabled">
**$H_0$: the proportion of fatalities for crashes that involve drunk**
**drivers is the same as the proportion of fatalities for crashes that do**
**not involve drunk drivers.**

$H_a$: **the proportion of fatalities for crashes that involve drunk**
**drivers is different than the proportion of fatalities for crashes that**
**do not involve drunk drivers.** 
</a>   


A high value of the $\chi^2$ statistic with a p-value \< 0.05, would
suggest that there is evidence to reject the null hypothesis for the
alternative. It would also suggest that there is an association between
drunk driving fand crash fatalities.

Before we run our Logistic Regression Model we should also examine
whether the means of the continuous predictors seem to differ for the
different levels of the dependent variable. The independent samples
t-test is the appropriate statistical test to do this, to examine if
there are significant differences in mean values of our continuous
variables, `PCTBACHMOR` and `MEDHHINC`, for crashes that involved drunk
driving and those that didn't. In the independent samples t-test, our
null and alternative hypotheses are as such:   

<a name = "ttest" class="isDisabled">
**$H_0$: average values of each continuous variable are the same for**
**crashes that involve drunk drivers and crashes that did not.**

**$H_a$: average values of each continuous variable are different for**
**crashes that involve drunk drivers and crashes that did not.**
</a>

A high value of the t-statistic with a p-value \< 0.05 would suggest
that there is evidence to reject the null hypothesis in favor of the
alternative.

We also took a look at the pearson correlations between all the
predictors (both binary and continuous) to determine if there was
evidence of severe multicollinearity.

# RESULTS

### EXPLORATORY ANALYSIS
<div style= "float:right; position: relative; padding: 15px;">
```{r DD.tab, message=FALSE, warning=FALSE, out.width= "65%"}
DRINKING_D.tab <- table(mydata$DRINKING_D)
prop.table<- prop.table(DRINKING_D.tab)

bind_cols(data.frame(Description = c(
  "No Alcohol Involved",
  "Alcohol Involved")))%>%
bind_cols(as.data.frame(DRINKING_D.tab)%>%
  cbind(as.data.frame(prop.table)$Freq))%>%
  kable(format = "html", 
        align = "rlrr", 
        caption = "Table 1",
        col.names = c('Drinking_D','','Count','Prop'))%>%
  column_spec(1,bold = TRUE,
              background = '#f1f1f1')%>%
  column_spec(2,bold = TRUE,
              background = '#f1f1f1')%>%
  kable_paper()%>%kable_styling(full_width=F,
                                bootstrap_options = "hover") 
```
</div>
We first examine the proportion of crashes that involve drunk driving in our observations. Table 1 shows the count and the proportion of crashes that involve drunk driving where 0 means no alcohol was involved and 1 means alcohol was involved. The proportion of crashes that don't involve alcohol, calculated by dividing the number of crashes not involving alcohol by the total number of crashes, is much higher at just over 94% compared to compared to those that do at just under 6%. 

We next look at the cross tabulations of our dependent variable with each binary predictor in Table 2. Here, we are examining the number and percentage of crashes not involving alcohol, and crashes involving alcohol where each predictor is true. For example, row one tells us that out of the 1300 crashes that resulted in fatality or major injury, 119 (0.27%) did not involve alcohol and 1181 (2.72%) did. For each predictor we ran the Chi-Square test to determine whether the distribution of our dependent categorical variable varies with respect to the values of the predictor categorical variables. The p-value for each predictor indicates whether or not we can reject or fail to reject our null hypothesis *(defined for `FATAL_OR_M` in [Methods](#chi))*. We see that each binary predictor has a p-value lower than 0.05 which suggests that there is evidence to reject the null hypothesis in favor of the alternative, and that there's an association between all of our predictor variables and the dependent variable except for `CELL_PHONE`. This means there is not enough variance in the values of our binary dependent variable with respect to drivers using a cell phone at the time of the crash.

```{r cross.tab, include = TRUE, warning=FALSE, message=FALSE,results='hide'}

#2.a
#Alternative way of tabulating (and obtaining proportions for each category)
DRINKING_D.tab <- table(mydata$DRINKING_D)
DRINKING_D.tab%>%kable(format = "html", align = "ll", caption = "Drunk Driving Counts")%>%kable_paper()%>%kable_styling(full_width=F) 
prop.table(DRINKING_D.tab)%>%kable(format = "html", align = "ll", caption = "Drunk Driving Proportion")%>%kable_paper()%>%kable_styling(full_width=F)
#proportion of crashes that involved a drunk driver



Cross_Fatal<-CrossTable(mydata$FATAL_OR_M, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_Fatal_tbl<-
  cbind(as.data.frame(Cross_Fatal$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_Fatal[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='FATAL_OR_M')
         

#.ii
Cross_Over<- CrossTable(mydata$OVERTURNED, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_Over_tbl<-
  cbind(as.data.frame(Cross_Over$t)%>%filter(x==1)%>%rename(N=Freq),
      as.data.frame(Cross_Over[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='OVERTURNED')

Cross_Cell<-CrossTable(mydata$CELL_PHONE, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_Cell_tbl<-
  cbind(as.data.frame(Cross_Cell$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_Cell[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='CELL_PHONE')

Cross_Speed<-CrossTable(mydata$SPEEDING, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_Speed_tbl<-
  cbind(as.data.frame(Cross_Speed$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_Speed[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='SPEEDING')

Cross_Agg<-CrossTable(mydata$AGGRESSIVE, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_Agg_tbl<-
  cbind(as.data.frame(Cross_Agg$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_Agg[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='AGGRESSIVE')

Cross_1617<-CrossTable(mydata$DRIVER1617, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_1617_tbl<-
  cbind(as.data.frame(Cross_1617$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_1617[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
          dplyr::select(-x1,-y1)%>%
          mutate(Variable='DRIVER1617')

Cross_65P<-CrossTable(mydata$DRIVER65PLUS, mydata$DRINKING_D, prop.r=FALSE, prop.t=FALSE, prop.chisq=FALSE, chisq=TRUE)
Cross_65P_tbl<-
  cbind(as.data.frame(Cross_65P$t)%>%filter(x==1)%>%rename(N=Freq),
        as.data.frame(Cross_65P[["prop.tbl"]])%>%filter(x==1)%>%rename('percentage'=Freq,x1=x,y1=y))%>%
  dplyr::select(-x1,-y1)%>%
  mutate(Variable='DRIVER65PLUS')

# Putting table together - there might be a simpler way of doing this but this ends up working
Cross_Joined<-rbind(Cross_Fatal_tbl,Cross_Over_tbl,Cross_Cell_tbl,Cross_Speed_tbl,Cross_Agg_tbl,Cross_1617_tbl,Cross_65P_tbl)

CrossTablex<- Cross_Joined %>%spread(x, Variable)%>%filter(y==0)%>%rename(N_1=N,'percentage_1' = 'percentage')
CrossTabley<- Cross_Joined %>%spread(x, Variable)%>%filter(y==1)%>%
  dplyr::select(-y,-'1')%>%
  cbind(CrossTablex)%>%dplyr::select(-y)%>%rename(Variable = '1')
CrossTable<-CrossTabley%>%mutate(Total_N = N+N_1,
         '%' = round((percentage *100),2),
         '%_1' = round((percentage_1 *100),2))%>% dplyr::select(-percentage, -percentage_1)%>%
  arrange(factor(Variable, levels = c('FATAL_OR_M', 'OVERTURNED', 
                                      'CELL_PHONE', 'SPEEDING', 
                                      'AGGRESSIVE', 'DRIVER1617', 
                                      'DRIVER65PLUS')))%>%
  bind_cols(data.frame(Description = c(
  "Crash resulted in fatality or major injury",
  "Crash involved an overturned vehicle",
  "Driver was using cell phone",
  "Crash involved speeding vat",
  "Crash involved aggressive driving",
  "Crash involved at least one driver who was 16 or 17 years old",
  "Crash involved at least one driver who was at least 65 years old")))%>%
  bind_cols(data.frame('X2 p.value' = c(
    Cross_Fatal[["chisq"]][["p.value"]],
    Cross_Over[["chisq"]][["p.value"]],
    Cross_Cell[["chisq"]][["p.value"]],
    Cross_Speed[["chisq"]][["p.value"]],
    Cross_Agg[["chisq"]][["p.value"]],
    Cross_1617[["chisq"]][["p.value"]],
    Cross_65P[["chisq"]][["p.value"]])))
```

```{r cross.tab.ouput,  include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5 }

CrossTable[, c(3,7,1,5,2,6,4,8)]%>%
  kable(col.names = c("Variable", "","N","%","N","%","N", "Chi-Square\np-value"),
        align = c('r','l','c','c','c','c','c','c'),
        caption = 'Table 2')%>%
  kable_paper()%>%
  add_header_above(c("","","No Alcohol Involved\n(Drinking_D = 0)" = 2, "Alcohol Involved\n(Drinking_D = 1)" = 2,"Total", ""),
                   align = c('r','c'))%>%
  kable_styling( bootstrap_options = "hover")%>%
  column_spec(1,bold = TRUE)%>%
  column_spec(5,border_left = TRUE)%>%
  column_spec(c(3:4),background = '#f1f1f1')%>%
  column_spec(c(5:6),background = '#e1e1e1')

```

<br>
  
Our two continuous predictors `PCTBACHMOR` and `MEDHHINC` are analyzed by calculating the mean and standard deviation for each outcome of our binary dependent variable (Table 3). We can see that there is little variation in the means for both continuous variables which is further confirmed by the p-values of the t-test.The t-test will be an indicator of whether our continuous variables are associated with the dependent variable. Both p-values in Table 3 are quite high meaning we fail to reject the [null hypothesis](#ttest) that the average values (Mean) for each continuous variable are the same for crashes that do not involve alcohol and crashes that do involve alcohol.

```{r sd.mean.tab, include = TRUE, warning=FALSE, message=FALSE,results='hide'}
#Means by group
meanbach<-as.data.frame(t(tapply(mydata$PCTBACHMOR, mydata$DRINKING_D, mean)))%>%mutate(Variable= 'PCTBACHMOR')%>%
  rename('mean.0'='0', 'mean.1'='1')
# 16.56986 16.61173 

meanHHinc<-as.data.frame(t(tapply(mydata$MEDHHINC, mydata$DRINKING_D, mean)))%>%mutate(Variable= 'MEDHHINC')%>%
  rename('mean.0'='0', 'mean.1'='1')
# 31483.05 31998.75 

#SD by group
sdbach<-as.data.frame(t(tapply(mydata$PCTBACHMOR, mydata$DRINKING_D, sd)))%>%mutate(Variable= 'PCTBACHMOR')%>%
  rename('sd.0'='0', 'sd.1'='1')
# 18.21426 18.72091

sdHHinc<-as.data.frame(t(tapply(mydata$MEDHHINC, mydata$DRINKING_D, sd)))%>%mutate(Variable= 'MEDHHINC')%>%
  rename('sd.0'='0', 'sd.1'='1')
# 16930.1 17810.5

#save pvalues for table (step iii). 
ttest_pvalues<-as.data.frame(c((t.test(mydata$PCTBACHMOR~mydata$DRINKING_D)$p.value),
                               (t.test(mydata$MEDHHINC~mydata$DRINKING_D)$p.value)))

#ii.
#Mean and SD table
MeanSD_tbl<- merge(x=(merge(x=meanHHinc, y=sdHHinc, all= TRUE)),
                   y=(merge(x=meanbach, y=sdbach,all=TRUE)),
                   all=TRUE)%>%arrange(desc(Variable))%>%
  bind_cols(data.frame(Description = c(
    "% with a bachelor's degree or more",
    "Median household income")))%>%
  bind_cols(ttest_pvalues)
```

```{r sd.mean.tabouput, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5 }
#output for markdown
MeanSD_tbl[, c(1,6,2,4,3,5,7)]%>%
  kable(col.names = c("Variable","","Mean","SD","Mean","SD","t-test p-value"),
                                     align = c('r','l','c','c','c','c','c'),
                   caption = 'Table 3')%>%
  kable_paper()%>%
  add_header_above(c("","","No Alcohol Involved\n(Drinkind_D = 0)" = 2,
                     "Alcohol Involved\n(Drinkind_D = 1)" = 2,""),
                   align = c('r','c'))%>%
  kable_styling( bootstrap_options = "hover")%>%
  column_spec(1,bold = TRUE)%>%
  column_spec(5,border_left = TRUE)%>%
  column_spec(c(3:4),background = '#f1f1f1')%>%
  column_spec(c(5:6),background = '#e1e1e1')
```

### LOGISTIC REGRESSION ASSUMPTIONS  

We meet our first assumption of logistic regression that our dependent variable is binary -- `DRINGING_D` has an output of either 1= Yes or 0 = No. Our predictors are independent of each other, and we create Table 4 to examine any indication of multicollinearity. We create a matrix showing the pairwise Pearson correlations for all predictors, binary and continuous, and check to see that we don't have any correlations where our r value is higher than 0.8 or less than -0.8. While none of our predictors have severe multicollinearity and we are cleared to include all predictors in our regression, our limitation is that we are not meeting the Pearson correlation assumption that our variables are continuous. Only two of our predictors in our matrix are continuous and, the rest of the predictors and the dependent variable are binary. We should not typically use the Pearson correlation for two binary variables. 

```{r corr.mat, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}
pred_var <- mydata%>%dplyr::select(DRINKING_D, FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING,
                                   AGGRESSIVE,DRIVER1617,DRIVER65PLUS,PCTBACHMOR,MEDHHINC)
pcorr <- cor(pred_var, method="pearson")

#correlation matrix for markdown
pcorr%>%kable(caption = 'Table 4')%>%
  kable_styling(full_width=FALSE,
                bootstrap_options = "hover")%>%
  kable_paper()%>%
  column_spec(1,bold = TRUE)
```

### LOGISTIC REGRESSION RESULTS

When using logistic regression to identify predictors of accidents
related to drunk driving, we employed two different models. The first
used all the predictors listed in the methods section. The results for
this model are shown below.

```{r logitcode, include=FALSE, echo=TRUE, warning=FALSE, results='hide'}
#Step 3 -------------------------------------



#Logistic Regression
mylogit <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS + PCTBACHMOR + MEDHHINC, data=mydata, family = "binomial")

# SAVING LOGSITIC REGRESSION OUTPUT AS LOGITOUTPUT
logitoutput<-summary(mylogit)

# SAVING COEFFS ESTIMATES, STANDARD ERROR, Z-VALUES, AND P-VALUES AS LOGITCOEFFS
logitcoeffs<-logitoutput$coefficients


# SAVING ODDS RATIONS AND 95% CONFIDENCE INTERVALS FOR THE ODDS RATIOS AS OR_CI
or_ci<- exp(cbind(OR=coef(mylogit), confint(mylogit)))

# APPENDING COEFFS WITH ODDS RATIOS AND CONFIDENCE INTERVALS
finallogitoutput<- cbind(logitcoeffs, or_ci)





```

```{r logittable, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}

# MAKING IT PRETTY

finallogitoutput%>%kable(format = "html", align = "rlllllll", caption = "Logistic Regression Results")%>%
  kable_paper("hover")%>%kable_styling(full_width=F)

```

Nearly all of the predictors appear to be significant except for
`PCTBACHMOR` and `CELLPHONE`. And whether the crash involved a speeding
car (`SPEEDING`), has the highest odds ratio. The odds of a crash
involving a drunk driver and a car that is speeding are 365.98% higher
than the odds of the crash involving a drunk driver but no car that is
speeding, holding all other variables constant. The predictors for
whether the crash involved a fatality or serious injury, as well as
whether a car involved in the crash overturned also have high odds
ratios. The odds of a crash involving a drunk driver and a fatality or
serious injury are 125.69% higher than the odds of a crash involving a
drunk driver and no serious injury or fatality, all else constant. And
the odds of a crash involving a drunk driver and an overturned car are
153.17% higher than the odds of a crash involving a drunk driver and no
overturned car, all else constant. The Median Household Income of the
census block group where the crash took place seems to be the only
significant continuous variable, but it's effect on the dependent
variable is not strong. For a one percent increase in `MEDHHINC`, the
odds of a crash involving a drunk driver go up by 0.00028%, all else
constant. The remaining significant predictors can be interpreted as
such:

**With all else constant, the odds of a crash involving a drunk driver
:**

-   and aggressive driving are 44.94% lower than the odds of crash
    involving a drunk driver and no aggressive driving

-   and at least one driver between 16 and 17 years old are 72.21% lower
    than the odds of crash involving a drunk driver and no driver
    between 16 and 17 years old

-   and at least one driver at least 65 years old are 53.91% lower than
    the odds of crash involving a drunk driver and no driver at least 65
    years old

Below is a table that shows the specificity, sensitivity, and
misclassification rates for the different probability cut-offs. The
cut-off that yields the lowest misclassification rate is the 0.0
threshold, highlighted in yellow. The 0.50 threshold yields a 0.0573506
misclassification rate. The cut-off that yields the highest
misclassification rate is the 0.02 threshold, the lowest threshold. The
0.02 threshold yields a misclassification rate of 0.8888940.

```{r spec.sens.mis.code, include=FALSE, echo=TRUE, warning=FALSE, echo=FALSE, results='hide'}

  ## 3.a.iii ---- 

#### bringing in iterate thresholds fn 
# Iterate Thresholds Chapter 6, 7 (left in Chapters)
iterateThresholds <- function(data, observedClass, predictedProbs, group) {
  #This function takes as its inputs, a data frame with an observed binomial class (1 or 0); a vector of predicted #probabilities; and optionally a group indicator like race. It returns accuracy plus counts and rates of confusion matrix #outcomes. It's a bit verbose because of the if (missing(group)). I don't know another way to make an optional parameter.
  observedClass <- enquo(observedClass)
  predictedProbs <- enquo(predictedProbs)
  group <- enquo(group)
  x = .01
  all_prediction <- data.frame()
  
  if (missing(group)) {
    
    while (x <= 1) {
      this_prediction <- data.frame()
      
      this_prediction <-
        data %>%
        mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
        count(predclass, !!observedClass) %>%
        summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
                  Count_TP = sum(n[predclass==1 & !!observedClass==1]),
                  Count_FN = sum(n[predclass==0 & !!observedClass==1]),
                  Count_FP = sum(n[predclass==1 & !!observedClass==0]),
                  Rate_TP = Count_TP / (Count_TP + Count_FN),
                  Rate_FP = Count_FP / (Count_FP + Count_TN),
                  Rate_FN = Count_FN / (Count_FN + Count_TP),
                  Rate_TN = Count_TN / (Count_TN + Count_FP),
                  Sensitivity = Count_TP / (Count_TP + Count_FP),
                  Specificity = Count_TN / (Count_TN + Count_FN),
                  MissClass_Rate = (Count_FP + Count_FN)/(Count_FP + Count_FN + Count_TP + Count_TN), 
                  Accuracy = (Count_TP + Count_TN) / 
                    (Count_TP + Count_TN + Count_FN + Count_FP)) %>%
        mutate(Threshold = round(x,2))
      
      all_prediction <- rbind(all_prediction,this_prediction)
      x <- x + .01
    }
    return(all_prediction)
  }
  else if (!missing(group)) { 
    while (x <= 1) {
      this_prediction <- data.frame()
      
      this_prediction <-
        data %>%
        mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
        group_by(!!group) %>%
        count(predclass, !!observedClass) %>%
        summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
                  Count_TP = sum(n[predclass==1 & !!observedClass==1]),
                  Count_FN = sum(n[predclass==0 & !!observedClass==1]),
                  Count_FP = sum(n[predclass==1 & !!observedClass==0]),
                  Rate_TP = Count_TP / (Count_TP + Count_FN),
                  Rate_FP = Count_FP / (Count_FP + Count_TN),
                  Rate_FN = Count_FN / (Count_FN + Count_TP),
                  Rate_TN = Count_TN / (Count_TN + Count_FP),
                  Sensitivity = Count_TP / (Count_TP + Count_FP),
                  Specificity = Count_TN / (Count_TN + Count_FN),
                  MissClass_Rate = (Count_FP + Count_FN)/(Count_FP + Count_FN + Count_TP + Count_TN),
                  Accuracy = (Count_TP + Count_TN) / 
                    (Count_TP + Count_TN + Count_FN + Count_FP)) %>%
        mutate(Threshold = round(x,2))
      
      all_prediction <- rbind(all_prediction,this_prediction)
      x <- x + .01
    }
    return(all_prediction)
  }
}

#prepping
fit <-mylogit$fitted.values

#a is a matrix combining the vectors containing y and y-hat in matrix a; first variable is
#DRINKING_D, which is y; second variable is fit, which is y-hat

a <- cbind(mydata$DRINKING_D, fit)

#b is matrix a, just sorted by the variable fit
b <- a[order(a[,2]),]
b=as.data.frame(b)
colnames(b) <- c("Observed.DRINKING_D","Probability.DRINKING_D")

#### Don't Need This Stuff, that is folded ----

#Calculating variable c which is 1 if y-hat (second column of matrix b) is greater
#than or equal to 0.05 and 0 otherwise.

#Other cut-offs can be used here!
c <- (b[,2] >= 0.05)


#Creating matrix d which merges matrixes b and c
d <- cbind(b,c)

#Let's label the columns of matrix d for easier reading
colnames(d) <- c("Observed.DRINKING_D","Probability.DRINKING_D","Prob.Above.Cutoff")

#Converting matrix to data frame
e=as.data.frame(d)

# ----

whichThreshold <- 
  iterateThresholds(
    data=b, observedClass = Observed.DRINKING_D, predictedProbs = Probability.DRINKING_D)


# looking for these threshes:

ourthresh <- whichThreshold %>%filter(Threshold == 0.02 | Threshold == 0.03|Threshold == 0.05|(Threshold > 0.06 & Threshold < 0.11)| Threshold == 0.15 | Threshold == 0.2 | Threshold == 0.5)%>%
  dplyr::select(Threshold, Sensitivity, Specificity, MissClass_Rate)

07

```

```{r spec.sens.mis.table, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}
ourthresh%>%kable(format = "html", align = "llll", caption = "Goodness of Fit Metrics",
                  col.names = c("Cut Off Value", "Sensitivity", "Specificity", "Missclassification Rate"))%>%
  kable_paper("hover")%>%kable_styling(full_width=F)%>% 
  row_spec(10, bold = T,background = "#f0d560")


```

Below is the resulting ROC curve and a table showing our optimal cutoff
and associated sensitivity and specificity values. Working with the goal
that specificity and sensitivity should be maximized, our threshold was
determined by identifying the point of the curve where it has the
minimum distance from the upper left corner of the graph, i.e. where
both specificity and sensitivity are equal to 1. We identified that the
optimal threshold is 0.06. This threshold is significantly lower than
the optimal threshold identified using only the misclassification rate
as a guide. The resulting sensitivity and specificity are both ...
[***IDK what is going on here. I tried to repurpose the iterate
thresholds fn from 508, so maybe that has something to do with this? The
problem is that the sensitivity and specificity shown in the optimal
cutoff table for threshold of 0.06 is wildly different than the
thresholds of 0.07 and 0.05 shown in the misclassification rate table. I
will investigate later.***]{.ul}

```{r roc.optthresh.code, include=FALSE, echo=TRUE, warning=FALSE, echo=FALSE, results='hide'}

 ## 3.a.iv-v ---- 
#ROC Curve 
#For more info, see: https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/

#Here, predictions are estimated probabilities (i.e., p or y-hat values)
#Also, labels are actual y-values


a <- cbind(mydata$DRINKING_D, fit)

#From above, we see that matrix a has 2 columns: 
  #1. The first one is mydata$DRINKING_D, which are actual 
  #values of y (i.e., labels)
  #2. The second one is fit, which are predicted, or fitted values
  #of y (i.e., predictions)

#Let's make the names of the variables easy to understand

colnames(a) <- c("labels","predictions")


roc <- as.data.frame(a)
pred <- prediction(roc$predictions, roc$labels)


  #Below, tpr = true positive rate, another term for sensitivity
  #fpr = false positive rate, or 1-specificity


#Optimal cut-point:
  #if you want to weigh specificity and sensitivity equally.
  #There are a couple ways to identify the optimal cut point.
  #One is the so-called Youden Index, which identifies the cut-off
  #point for which (sensitivity + specificity) is maximized.
  
  #Another one, calculated using the code below, is the cut-off
  #value for which the ROC curve has the minimum distance from
  #the upper left corner of the graph, where specificity = 1 and
  #sensitivity = 1. (This is just a different way of maximizing 
  #specificity and sensitivity). This is where the 
  #d = (x - 0)^2 + (y-1)^2
  #in the code below comes in.

opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}





```

```{r roc.optthresh.output, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}

  #Below, tpr = true positive rate, another term for sensitivity
  #fpr = false positive rate, or 1-specificity

roc.perf = performance(pred, measure = "tpr", x.measure="fpr")
plot(roc.perf)
abline(a=0,b=1)


  #This will print the optimal cut-off point and the corresponding
  #specificity and sensitivity 

opt.cut(roc.perf, pred)%>%kable(format = "html", align = "ll", caption = "Optimal Cutoff")%>%
  kable_paper("hover")%>%kable_styling(full_width=F)%>%row_spec(3, bold=TRUE, background = "#f0d560")



```

Additionally, the resulting AUC for the ROC curve is 0.6398 and that
tells us that our model is fair, but not great.

```{r auc, include = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}
auc.perf = performance(pred, measure ="auc")
auc.perf@y.values%>%kable(col.names= c("Area Under the Curve"), format = "html", align = "c")%>%kable_material()%>%kable_styling(full_width=F, position = "center")

```

Below are the results from the second model that incorporated all the
same predictors as the first model except `PCTBACHMOR` and `MEDHHINC`.
In this variation it appears that all of the same variable are
significant, the only one not significant is `CELLPHONE`.

```{r logit.round2.aic.code, include=FALSE, echo=TRUE, warning=FALSE, echo=FALSE, results='hide'}


#Step 3 ROUND TWO  -------------------------------------

## 3.a.i ROUND TWO ---- 
#Logistic Regression
mylogit2 <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS, data=mydata, family = "binomial")

## 3.a.ii ROUND TWO---- 
# SAVING LOGSITIC REGRESSION OUTPUT AS LOGITOUTPUT
logitoutput2<-summary(mylogit2)

# SAVING COEFFS ESTIMATES, STANDARD ERROR, Z-VALUES, AND P-VALUES AS LOGITCOEFFS
logitcoeffs2<-logitoutput2$coefficients


# SAVING ODDS RATIONS AND 95% CONFIDENCE INTERVALS FOR THE ODDS RATIOS AS OR_CI
or_ci2<- exp(cbind(OR=coef(mylogit2), confint(mylogit2)))

# APPENDING COEFFS WITH ODDS RATIOS AND CONFIDENCE INTERVALS
finallogitoutput2<- cbind(logitcoeffs2, or_ci2)

## Final Step AIC -------------------------------------------


aic <- AIC(mylogit, mylogit2)%>%as.data.frame()
rownames(aic) <- NULL   


```

```{r logit.round2.output, nclude = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}

# MAKING IT PRETTY

finallogitoutput2%>%kable(format = "html", align = "rlllllll", caption = "Logistic Regression Results: Round Two")%>%
  kable_paper("hover")%>%kable_styling(full_width=F)

```

We also examined if either of the models were better than the other
using the Akaike Information Criterion. The the difference in AIC
between the two models is not larger than three, so we cannot say that
one of our models is better than the other.

```{r aic.output, nclude = TRUE, echo= FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=6}

aic%>%mutate(Model = c("Logit 1", "Logit 2"))%>% rename("Degrees of Freedom" = df)%>%
  dplyr::select(Model, "Degrees of Freedom", AIC)%>%kable(format = "html", align = "lll", caption = "AIC Results")%>%
  kable_paper("hover")%>%kable_styling(full_width=F)

```

# DISCUSSION

To examine the predictors of car crashes cause by alcohol in Philadelphia we ran logistic regression. Our dependent variable, whether or not a crash observation included an alcohol impaired driver, was binary, and we included a combination of binary and continuous predictive variables. We first explore our data by testing the associations between our predictors and the dependent variable through the Chi-Square test for our binary predictors, and the t-test for our categorical predictors. We then ran a Pearson correlation test to rule out any chance of multicollinearity among our variables. Our first logistic regression model uses all categorical and binary predictors we test in our exploratory analysis, in which we found `PCTBACHMOR` and `CELLPHONE` are not significant predictors, and `MEDHHINC` - the median household income of the block group in which the car crash took place - has very marginal predictive power over whether a car crash will include an alcohol impaired driver. We analyzed probability cutoffs to assess which threshold will have the lowest misclassification rate, plotted an ROC curve and calculated the AUC to identify our optimal threshold of 0.06. **!!CHECK!!** Our second regression included only our binary variables and excluded our Census data, and yielded similar results with all predictors being significant with the exception of `CELLPHONE`. We concluded that the Akaike Information Criterion of both models are similar and we cannot say either model is better than the other. 


**b) What are some limitations of the analysis? Discuss.**
